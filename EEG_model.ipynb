{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eb587d4c-7016-49c9-839b-5a741a63c672",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] The system cannot find the path specified: 'C:\\\\Users\\\\xmoot\\\\Desktop\\\\VSCode\\\\gsp-eeg-alz\\\\features_tv.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 21\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39msqrt(TV)\n\u001b[0;32m     20\u001b[0m dir_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mxmoot\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mDesktop\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mVSCode\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mgsp-eeg-alz\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mfeatures_tv.csv\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m---> 21\u001b[0m file_list \u001b[38;5;241m=\u001b[39m [entry\u001b[38;5;241m.\u001b[39mpath \u001b[38;5;28;01mfor\u001b[39;00m entry \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39mscandir(dir_path) \u001b[38;5;28;01mif\u001b[39;00m entry\u001b[38;5;241m.\u001b[39mis_file() \u001b[38;5;129;01mand\u001b[39;00m entry\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.set\u001b[39m\u001b[38;5;124m\"\u001b[39m)]\n\u001b[0;32m     23\u001b[0m n_files \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(file_list)\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_files \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] The system cannot find the path specified: 'C:\\\\Users\\\\xmoot\\\\Desktop\\\\VSCode\\\\gsp-eeg-alz\\\\features_tv.csv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import mne\n",
    "import os\n",
    "from pygsp import graphs, utils\n",
    "from scipy.spatial import distance_matrix\n",
    "from scipy.stats import entropy\n",
    "import networkx as nx\n",
    "from sklearn.cluster import spectral_clustering\n",
    "def compute_total_variation(W, data_values):\n",
    "    N = data_values.shape[0]\n",
    "    TV = 0\n",
    "    for i in range(N):\n",
    "        for j in range(N):\n",
    "            wij = W[i, j]\n",
    "            difference = data_values[j] - data_values[i]\n",
    "            TV += wij * np.linalg.norm(difference)**2\n",
    "    return np.sqrt(TV)\n",
    "\n",
    "dir_path = r'C:\\Users\\xmoot\\Desktop\\VSCode\\gsp-eeg-alz\\features_tv.csv'\n",
    "file_list = [entry.path for entry in os.scandir(dir_path) if entry.is_file() and entry.path.endswith(\".set\")]\n",
    "\n",
    "n_files = len(file_list)\n",
    "if n_files == 0:\n",
    "    raise ValueError(f\"No .set files found in directory {dir_path}.\")\n",
    "\n",
    "print(f'Found {n_files} .set files.')\n",
    "\n",
    "channel_names = ['Fp1', 'Fp2', 'F3', 'F4', 'C3', 'C4', 'P3', 'P4', 'O1', 'O2', 'F7', 'F8', 'T3', 'T4', 'T5', 'T6', 'Fz', 'Cz', 'Pz']\n",
    "\n",
    "data_list = []\n",
    "features = {}\n",
    "\n",
    "for i, file in enumerate(file_list):\n",
    "    raw = mne.io.read_raw_eeglab(file)\n",
    "    data = raw.get_data(picks=channel_names)\n",
    "    transposed_data = np.transpose(data)\n",
    "    data = pd.DataFrame(transposed_data, columns=channel_names)\n",
    "    data = data.groupby(data.index // 50).median()\n",
    "    data_list.append(data)\n",
    "\n",
    "    # GSP analysis\n",
    "    distances = distance_matrix(data.values, data.values)\n",
    "    theta, k = 1.0, 1.0 \n",
    "    W = np.exp(-distances**2 / theta**2)\n",
    "    W[distances > k] = 0\n",
    "    np.fill_diagonal(W, 0)\n",
    "    G = graphs.Graph(W)\n",
    "    L = G.L.toarray()\n",
    "    eigenvalues, eigenvectors = np.linalg.eigh(L)\n",
    "    X_GdataT = eigenvectors.T @ data.values\n",
    "    C = np.cov(X_GdataT)\n",
    "    T = eigenvectors.T.conj() @ C @ eigenvectors\n",
    "    r = np.linalg.norm(np.diag(T)) / np.linalg.norm(T, 'fro')\n",
    "    P = L @ data.values\n",
    "    Y = np.sum(data.values * P)**2\n",
    "    TV = compute_total_variation(W, data.values)\n",
    "\n",
    "    # Spectral Graph Features\n",
    "    graph_energy = np.sum(np.abs(eigenvalues))\n",
    "    spectral_entropy = entropy(np.square(eigenvectors))\n",
    "\n",
    "    # Graph Signal Features\n",
    "    signal_energy = np.sum(np.square(data.values))\n",
    "    signal_power = np.var(data.values)\n",
    "\n",
    "    # Graph Modularity and Community Structure\n",
    "    labels = spectral_clustering(W)\n",
    "    unique_labels = len(np.unique(labels))\n",
    "\n",
    "    # Graph Degree Distribution\n",
    "    degree_distribution = np.sum(W, axis=0)\n",
    "\n",
    "    # Graph Diffusion Characteristics\n",
    "    heat_trace = np.trace(np.exp(-L))\n",
    "    diffusion_distance = np.sum(np.exp(-L))\n",
    "\n",
    "    # Aggregating Features\n",
    "    features[os.path.basename(file)] = {\n",
    "        'stationary_ratio': r, \n",
    "        'Tik-norm': Y, \n",
    "        'Total_Variation': TV,\n",
    "        'graph_energy': graph_energy,\n",
    "        'spectral_entropy': spectral_entropy,\n",
    "        'signal_energy': signal_energy,\n",
    "        'signal_power': signal_power,\n",
    "        'unique_clusters': unique_labels,\n",
    "        'avg_degree': np.mean(degree_distribution),\n",
    "        'heat_trace': heat_trace,\n",
    "        'diffusion_distance': diffusion_distance\n",
    "    }\n",
    "\n",
    "features_data = pd.DataFrame(features).T\n",
    "features_data.to_csv('features_tv.csv', index_label='participant_id')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "features = pd.read_csv(\"features_tv.csv\", )\n",
    "participants = pd.read_csv(\"participants.tsv\", delimiter='\\t')\n",
    "data = features.merge(participants, left_index=True, right_index=True)\n",
    "\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a robust scaler object\n",
    "scaler = RobustScaler()\n",
    "\n",
    "# Fit the scaler to the data and transform it\n",
    "data[['stationary_ratio', 'Tik-norm']] = scaler.fit_transform(data[['stationary_ratio', 'Tik-norm']])\n",
    "\n",
    "# Set the aesthetic style of the plot\n",
    "sns.set(style=\"whitegrid\", font_scale=1.2, rc={\"grid.linewidth\": 0.5})\n",
    "\n",
    "# Create a color palette with three colors\n",
    "palette = sns.color_palette(\"Set2\", n_colors=3)\n",
    "\n",
    "# Use seaborn lmplot function to generate the scatter plot\n",
    "plot = sns.lmplot(data=data, x='stationary_ratio', y='Tik-norm', hue='Group', palette=palette,\n",
    "                  fit_reg=False, legend=False, scatter_kws={'s': 50, 'alpha': 0.7}, height=5, aspect=1.15)  # Adjust point size and transparency\n",
    "\n",
    "plot.set_axis_labels('Stationary Ratio', 'Tik-norm', fontsize=16)  # Set new axis labels and increase font size\n",
    "\n",
    "# Change legend labels\n",
    "new_labels = ['AD', 'HC', 'FTD']\n",
    "legend = plt.legend(title='Group', labels=new_labels, title_fontsize='16', fontsize='14', loc='center left', bbox_to_anchor=(1, 0.5), frameon=False)\n",
    "legend.get_title().set_fontsize('16')  # Set the fontsize of the legend title\n",
    "\n",
    "# Remove title\n",
    "plt.title('')\n",
    "\n",
    "# Adjust layout so legend does not get cut off\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.savefig('enhanced_plot.png', dpi=300, bbox_inches='tight')  \n",
    "from sklearn.preprocessing import RobustScaler\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a new column that maps 'A' and 'F' to the same value\n",
    "data['Group_color'] = data['Group'].map({'A': 'Group1', 'F': 'Group1', 'C': 'Group2'})\n",
    "\n",
    "# Create a robust scaler object\n",
    "scaler = RobustScaler()\n",
    "\n",
    "# Fit the scaler to the data and transform it\n",
    "data[['stationary_ratio', 'Tik-norm']] = scaler.fit_transform(data[['stationary_ratio', 'Tik-norm']])\n",
    "\n",
    "# Set the aesthetic style of the plot\n",
    "sns.set(style=\"whitegrid\", font_scale=1.2, rc={\"grid.linewidth\": 0.5})\n",
    "\n",
    "# Create a color palette with two colors since there are two groups now\n",
    "palette = sns.color_palette(\"Set2\", n_colors=2)\n",
    "\n",
    "# Use seaborn lmplot function to generate the scatter plot\n",
    "plot = sns.lmplot(data=data, x='stationary_ratio', y='Tik-norm', hue='Group_color', palette=palette,\n",
    "                  fit_reg=False, legend=False, scatter_kws={'s': 50, 'alpha': 0.7},\n",
    "                  height=5, aspect=1.15)\n",
    "plot.set_axis_labels('Stationary Ratio', 'Tik-norm', fontsize=16)  # Set new axis labels and increase font size\n",
    "\n",
    "# Change legend labels\n",
    "new_labels = ['Dementia', 'HC']\n",
    "legend = plt.legend(title='Group', labels=new_labels, title_fontsize='16', fontsize='14', loc='center left', bbox_to_anchor=(1, 0.5), frameon=False)\n",
    "legend.get_title().set_fontsize('16')  # Set the fontsize of the legend title\n",
    "\n",
    "# Remove title\n",
    "plt.title('')\n",
    "\n",
    "# Adjust layout so legend does not get cut off\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the plot to a file\n",
    "plt.savefig('enhanced_plot.png', dpi=300, bbox_inches='tight')  # Optionally, display the plot\n",
    "plt.show()\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "# Assuming 'data' is a DataFrame that contains the columns 'Group' and the features we want to plot\n",
    "columns_to_scale = ['stationary_ratio', 'Tik-norm', 'Total_Variation', 'graph_energy', 'spectral_entropy', 'signal_energy', 'signal_power', 'unique_clusters', 'avg_degree', 'heat_trace', 'diffusion_distance', 'MMSE', 'Age']\n",
    "\n",
    "# Detect problematic columns (i.e., columns that contain strings which look like lists)\n",
    "problematic_columns = [col for col in columns_to_scale if isinstance(data[col].iloc[0], str)]\n",
    "\n",
    "# Convert those columns\n",
    "for col in problematic_columns:\n",
    "    # Modify the string to have commas and then convert to list\n",
    "    data[col] = data[col].apply(lambda x: eval('[' + ','.join(x.strip('[]').split()) + ']')[0] if isinstance(x, str) else x)\n",
    "\n",
    "# Create a robust scaler object\n",
    "scaler = RobustScaler()\n",
    "\n",
    "# Scale all columns\n",
    "data[columns_to_scale] = scaler.fit_transform(data[columns_to_scale])\n",
    "\n",
    "features_to_plot = {\n",
    "    'stationary_ratio': 'Stationary Ratio',\n",
    "    'Tik-norm': 'Tik-norm',\n",
    "    'Total_Variation': 'Total Variation',\n",
    "    'graph_energy': 'Graph Energy',\n",
    "    'spectral_entropy': 'Spectral Entropy',\n",
    "    'signal_energy': 'Signal Energy',\n",
    "    'signal_power': 'Signal Power',\n",
    "    'unique_clusters': 'Unique Clusters',\n",
    "    'avg_degree': 'Average Degree',\n",
    "    'heat_trace': 'Heat Trace',\n",
    "    'diffusion_distance': 'Diffusion Distance',\n",
    "    'MMSE': 'MMSE',\n",
    "    'Age': 'Age'\n",
    "}\n",
    "\n",
    "group_order = ['C', 'F', 'A']\n",
    "\n",
    "for feature, title in features_to_plot.items():\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.boxplot(x=\"Group\", y=feature, data=data, order=group_order)\n",
    "    plt.title(f'Box Plot of {title} by Group')\n",
    "    plt.show()\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "# Assuming 'data' is a DataFrame that contains the columns 'Group' and the features we want to plot\n",
    "columns_to_scale = ['stationary_ratio', 'Tik-norm', 'Total_Variation', 'graph_energy', 'spectral_entropy', 'signal_energy', 'signal_power', 'unique_clusters', 'avg_degree', 'heat_trace', 'diffusion_distance', 'MMSE', 'Age']\n",
    "\n",
    "# Detect problematic columns (i.e., columns that contain strings which look like lists)\n",
    "problematic_columns = [col for col in columns_to_scale if isinstance(data[col].iloc[0], str)]\n",
    "\n",
    "# Convert those columns\n",
    "for col in problematic_columns:\n",
    "    # Modify the string to have commas and then convert to list\n",
    "    data[col] = data[col].apply(lambda x: eval('[' + ','.join(x.strip('[]').split()) + ']')[0] if isinstance(x, str) else x)\n",
    "\n",
    "# Create a robust scaler object\n",
    "scaler = RobustScaler()\n",
    "\n",
    "# Scale all columns\n",
    "data[columns_to_scale] = scaler.fit_transform(data[columns_to_scale])\n",
    "\n",
    "features_to_plot = {\n",
    "    'stationary_ratio': 'Stationary Ratio',\n",
    "    'Tik-norm': 'Tik-norm',\n",
    "    'Total_Variation': 'Total Variation',\n",
    "    'graph_energy': 'Graph Energy',\n",
    "    'spectral_entropy': 'Spectral Entropy',\n",
    "    'signal_energy': 'Signal Energy',\n",
    "    'signal_power': 'Signal Power',\n",
    "    'unique_clusters': 'Unique Clusters',\n",
    "    'avg_degree': 'Average Degree',\n",
    "    'heat_trace': 'Heat Trace',\n",
    "    'diffusion_distance': 'Diffusion Distance',\n",
    "    'MMSE': 'MMSE',\n",
    "    'Age': 'Age'\n",
    "}\n",
    "\n",
    "\n",
    "data['Group'] = data['Group'].replace({'C': 'HC', 'F': 'FTD', 'A': 'AD'})\n",
    "group_order = ['HC', 'FTD', 'AD']\n",
    "\n",
    "for feature, title in features_to_plot.items():\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    sns.boxplot(x=\"Group\", y=feature, data=data, order=group_order)\n",
    "    plt.ylabel(features_to_plot[feature], fontsize=20)\n",
    "    plt.xlabel(\"Group\", fontsize=20)\n",
    "    plt.xticks(fontsize=18, rotation=0)\n",
    "    plt.yticks(fontsize=18)\n",
    "    plt.title(\"\")\n",
    "    plt.subplots_adjust(left=0.1, right=0.9, top=0.9, bottom=0.1)\n",
    "    plt.show()\n",
    "import umap\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Extract the columns you want from the data DataFrame\n",
    "\n",
    "features = {\n",
    "    'stationary_ratio': 'Stationary Ratio',\n",
    "    'Tik-norm': 'Tik-norm',\n",
    "    'Total_Variation': 'Total Variation',\n",
    "    'graph_energy': 'Graph Energy',\n",
    "    'spectral_entropy': 'Spectral Entropy',\n",
    "    'signal_energy': 'Signal Energy',\n",
    "    'signal_power': 'Signal Power',\n",
    "    'avg_degree': 'Average Degree',\n",
    "    'diffusion_distance': 'Diffusion Distance',\n",
    "}\n",
    "\n",
    "selected_data = data[list(features.keys())]\n",
    "\n",
    "# Create the UMAP object and fit_transform the data to get a 2D representation\n",
    "reducer = umap.UMAP()\n",
    "embedding = reducer.fit_transform(selected_data)\n",
    "\n",
    "# Define a custom color palette\n",
    "color_palette = {\"HC\": \"blue\", \"FTD\": \"red\", \"AD\": \"green\"}\n",
    "\n",
    "# Plot the UMAP representation\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.scatterplot(\n",
    "    x=embedding[:, 0], \n",
    "    y=embedding[:, 1], \n",
    "    hue=data['Group'],\n",
    "    palette=color_palette, \n",
    "    s=60,\n",
    "    alpha=0.8# Increased size\n",
    ")\n",
    "plt.gca().set_aspect('equal', 'datalim')\n",
    "plt.title(\"\")\n",
    "plt.legend(loc=\"upper right\", fontsize=16)\n",
    "plt.xlabel('UMAP 1', fontsize=18)\n",
    "plt.ylabel('UMAP 2', fontsize=18)\n",
    "plt.gca().tick_params(axis='both', which='major', labelsize=16)\n",
    "plt.show()    \n",
    "import umap\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "# Extract the columns you want from the data DataFrame\n",
    "features = {\n",
    "    'stationary_ratio': 'Stationary Ratio',\n",
    "    'Tik-norm': 'Tik-norm',\n",
    "    'Total_Variation': 'Total Variation',\n",
    "    'graph_energy': 'Graph Energy',\n",
    "    'spectral_entropy': 'Spectral Entropy',\n",
    "    'signal_energy': 'Signal Energy',\n",
    "    'signal_power': 'Signal Power',\n",
    "    'avg_degree': 'Average Degree',\n",
    "    'diffusion_distance': 'Diffusion Distance',\n",
    "}\n",
    "\n",
    "color_mapping = {\"AD\": \"#3498db\", \"HC\": \"#2ecc71\", \"FTD\": \"#e74c3c\"}  # Blue, Green, Red\n",
    "\n",
    "\n",
    "data['Group'] = data['Group'].map({'A': 'AD', 'C': 'HC', 'F': 'FTD'})\n",
    "\n",
    "\n",
    "selected_data = data[list(features.keys())]\n",
    "\n",
    "# Create a robust scaler object and fit-transform the data\n",
    "scaler = RobustScaler()\n",
    "scaled_data = scaler.fit_transform(selected_data)\n",
    "\n",
    "# Create the UMAP object and fit_transform the data to get a 2D representation\n",
    "reducer = umap.UMAP()\n",
    "embedding = reducer.fit_transform(scaled_data)\n",
    "\n",
    "# Set the aesthetic style of the plot\n",
    "sns.set(style=\"whitegrid\", font_scale=1.2, rc={\"grid.linewidth\": 0.5})\n",
    "\n",
    "# Create a color palette with three colors\n",
    "palette = sns.color_palette([\"#3498db\", \"#2ecc71\", \"#e74c3c\"])  # Blue, Green, Red\n",
    "\n",
    "# Plot the UMAP representation\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.scatterplot(\n",
    "    x=embedding[:, 0], \n",
    "    y=embedding[:, 1], \n",
    "    hue=data['Group'],\n",
    "    palette=palette, \n",
    "    s=50,  # Increased size\n",
    "    alpha=0.7  # Adjusted alpha\n",
    ")\n",
    "plt.gca().set_aspect('equal', 'datalim')\n",
    "\n",
    "# Set new axis labels and increase font size\n",
    "plt.xlabel('UMAP 1', fontsize=16)\n",
    "plt.ylabel('UMAP 2', fontsize=16)\n",
    "\n",
    "# Change legend labels\n",
    "new_labels = ['AD', 'HC', 'FTD']\n",
    "legend = plt.legend(title='Group', labels=new_labels, title_fontsize='16', fontsize='14', loc='center left', bbox_to_anchor=(1, 0.5), frameon=False)\n",
    "legend.get_title().set_fontsize('16')  # Set the fontsize of the legend title\n",
    "\n",
    "# Remove title\n",
    "plt.title('')\n",
    "\n",
    "# Adjust layout so legend does not get cut off\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the plot\n",
    "plt.savefig('umap_plot.png', dpi=300, bbox_inches='tight')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "\n",
    "features = {\n",
    "    'stationary_ratio': 'Stationary Ratio',\n",
    "    'Tik-norm': 'Tik-norm',\n",
    "    'Total_Variation': 'Total Variation',\n",
    "    'graph_energy': 'Graph Energy',\n",
    "    'spectral_entropy': 'Spectral Entropy',\n",
    "    'signal_energy': 'Signal Energy',\n",
    "    'signal_power': 'Signal Power',\n",
    "    'avg_degree': 'Average Degree',\n",
    "    'diffusion_distance': 'Diffusion Distance',\n",
    "}\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.metrics import classification_report, roc_curve, auc\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# The features are 'stationary_ratio' and 'Tik_norm'\n",
    "X = data[list(features.keys())]\n",
    "\n",
    "# The target is 'Group'\n",
    "y = data['Group']\n",
    "\n",
    "# Split the data into train+validation set and test set\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Further split the train data into train set and validation set\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.5, random_state=42)\n",
    "\n",
    "scaler = RobustScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Encode labels\n",
    "le = LabelEncoder()\n",
    "y_train_int = le.fit_transform(y_train)\n",
    "y_val_int = le.transform(y_val)\n",
    "y_test_int = le.transform(y_test)\n",
    "\n",
    "# Hyperparameters\n",
    "params = {\n",
    "    'n_neighbors': [3, 5, 7, 9, 11, 13],\n",
    "    'weights': ['uniform', 'distance'],\n",
    "    'metric': ['euclidean', 'manhattan', 'minkowski', 'canberra']\n",
    "}\n",
    "\n",
    "clf = KNeighborsClassifier(n_jobs=-1)\n",
    "grid_search = GridSearchCV(clf, params, cv=5, n_jobs=-1)\n",
    "grid_search.fit(X_train_scaled, y_train_int)\n",
    "\n",
    "best_clf = grid_search.best_estimator_\n",
    "\n",
    "# Validate and test metrics\n",
    "for label, group_name in enumerate(le.classes_):\n",
    "    y_val_label = (y_val_int == label).astype(int)\n",
    "    y_test_label = (y_test_int == label).astype(int)\n",
    "    y_val_pred_prob = best_clf.predict_proba(X_val_scaled)[:, label]\n",
    "    y_test_pred_prob = best_clf.predict_proba(X_test_scaled)[:, label]\n",
    "\n",
    "    fpr_val, tpr_val, _ = roc_curve(y_val_label, y_val_pred_prob)\n",
    "    fpr_test, tpr_test, _ = roc_curve(y_test_label, y_test_pred_prob)\n",
    "\n",
    "    print(f\"Validation ROC AUC for class {group_name} (Class {label}): {auc(fpr_val, tpr_val)}\")\n",
    "    print(f\"Test ROC AUC for class {group_name} (Class {label}): {auc(fpr_test, tpr_test)}\")\n",
    "\n",
    "y_val_pred = le.inverse_transform(best_clf.predict(X_val_scaled))\n",
    "y_test_pred = le.inverse_transform(best_clf.predict(X_test_scaled))\n",
    "\n",
    "print(\"\\nValidation Precision, Recall, F1-score:\\n\", classification_report(y_val, y_val_pred))\n",
    "print(\"\\nTest Precision, Recall, F1-score:\\n\", classification_report(y_test, y_test_pred))\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.metrics import classification_report, roc_curve, auc\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# The features are 'stationary_ratio' and 'Tik_norm'\n",
    "X = data[list(features.keys())]\n",
    "\n",
    "# The target is 'Group'\n",
    "y = data['Group']\n",
    "\n",
    "# Split the data into train+validation set and test set\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Further split the train data into train set and validation set\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.5, random_state=42)\n",
    "\n",
    "scaler = RobustScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Encode labels\n",
    "le = LabelEncoder()\n",
    "y_train_int = le.fit_transform(y_train)\n",
    "y_val_int = le.transform(y_val)\n",
    "y_test_int = le.transform(y_test)\n",
    "\n",
    "# Hyperparameters\n",
    "params = {\n",
    "    'n_estimators': [50, 100, 150, 200, 250, 300, 350],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "clf = RandomForestClassifier(random_state=42, n_jobs=-1)\n",
    "grid_search = GridSearchCV(clf, params, cv=5, n_jobs=-1)\n",
    "grid_search.fit(X_train_scaled, y_train_int)\n",
    "\n",
    "best_clf = grid_search.best_estimator_\n",
    "\n",
    "# Validate and test metrics\n",
    "for label, group_name in enumerate(le.classes_):\n",
    "    y_val_label = (y_val_int == label).astype(int)\n",
    "    y_test_label = (y_test_int == label).astype(int)\n",
    "    y_val_pred_prob = best_clf.predict_proba(X_val_scaled)[:, label]\n",
    "    y_test_pred_prob = best_clf.predict_proba(X_test_scaled)[:, label]\n",
    "\n",
    "    fpr_val, tpr_val, _ = roc_curve(y_val_label, y_val_pred_prob)\n",
    "    fpr_test, tpr_test, _ = roc_curve(y_test_label, y_test_pred_prob)\n",
    "\n",
    "    print(f\"Validation ROC AUC for class {group_name} (Class {label}): {auc(fpr_val, tpr_val)}\")\n",
    "    print(f\"Test ROC AUC for class {group_name} (Class {label}): {auc(fpr_test, tpr_test)}\")\n",
    "\n",
    "y_val_pred = le.inverse_transform(best_clf.predict(X_val_scaled))\n",
    "y_test_pred = le.inverse_transform(best_clf.predict(X_test_scaled))\n",
    "\n",
    "print(\"\\nValidation Precision, Recall, F1-score:\\n\", classification_report(y_val, y_val_pred))\n",
    "print(\"\\nTest Precision, Recall, F1-score:\\n\", classification_report(y_test, y_test_pred))\n",
    "\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.metrics import classification_report, roc_curve, auc\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# The features are 'stationary_ratio' and 'Tik_norm'\n",
    "X = data[list(features.keys())]\n",
    "\n",
    "# The target is 'Group'\n",
    "y = data['Group']\n",
    "\n",
    "# Split the data into train+validation set and test set\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Further split the train data into train set and validation set\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.5, random_state=42)\n",
    "\n",
    "scaler = RobustScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Encode labels\n",
    "le = LabelEncoder()\n",
    "y_train_int = le.fit_transform(y_train)\n",
    "y_val_int = le.transform(y_val)\n",
    "y_test_int = le.transform(y_test)\n",
    "\n",
    "# Hyperparameters\n",
    "params = {\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'max_depth': [3, 6, 10],\n",
    "    'n_estimators': [50, 100, 150, 200, 250, 300, 350, 400],\n",
    "    'subsample': [0.8, 1.0],\n",
    "    'colsample_bytree': [0.6, 0.8, 1.0]\n",
    "}\n",
    "\n",
    "clf = xgb.XGBClassifier(random_state=42, eval_metric='mlogloss')\n",
    "grid_search = GridSearchCV(clf, params, cv=5, n_jobs=-1)\n",
    "grid_search.fit(X_train_scaled, y_train_int)\n",
    "\n",
    "best_clf = grid_search.best_estimator_\n",
    "\n",
    "# Validate and test metrics\n",
    "for label, group_name in enumerate(le.classes_):\n",
    "    y_val_label = (y_val_int == label).astype(int)\n",
    "    y_test_label = (y_test_int == label).astype(int)\n",
    "    y_val_pred_prob = best_clf.predict_proba(X_val_scaled)[:, label]\n",
    "    y_test_pred_prob = best_clf.predict_proba(X_test_scaled)[:, label]\n",
    "\n",
    "    fpr_val, tpr_val, _ = roc_curve(y_val_label, y_val_pred_prob)\n",
    "    fpr_test, tpr_test, _ = roc_curve(y_test_label, y_test_pred_prob)\n",
    "\n",
    "    print(f\"Validation ROC AUC for class {group_name} (Class {label}): {auc(fpr_val, tpr_val)}\")\n",
    "    print(f\"Test ROC AUC for class {group_name} (Class {label}): {auc(fpr_test, tpr_test)}\")\n",
    "\n",
    "y_val_pred = le.inverse_transform(best_clf.predict(X_val_scaled))\n",
    "y_test_pred = le.inverse_transform(best_clf.predict(X_test_scaled))\n",
    "\n",
    "print(\"\\nValidation Precision, Recall, F1-score:\\n\", classification_report(y_val, y_val_pred))\n",
    "print(\"\\nTest Precision, Recall, F1-score:\\n\", classification_report(y_test, y_test_pred))\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, roc_curve, auc\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "# The features are 'stationary_ratio' and 'Tik-norm'\n",
    "X = data[list(features.keys())]\n",
    "\n",
    "# The target is 'Group'\n",
    "y = data['Group']\n",
    "\n",
    "# Split the data into train+validation set and test set\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Further split the train data into train set and validation set\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.5, random_state=42)\n",
    "\n",
    "scaler = RobustScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Encode labels\n",
    "le = LabelEncoder()\n",
    "y_train_int = le.fit_transform(y_train)\n",
    "y_val_int = le.transform(y_val)\n",
    "y_test_int = le.transform(y_test)\n",
    "\n",
    "clf = LogisticRegression(random_state=42, multi_class='auto', solver='liblinear', n_jobs=-1)\n",
    "clf.fit(X_train_scaled, y_train_int)\n",
    "\n",
    "# Validate and test metrics\n",
    "for label, group_name in enumerate(le.classes_):\n",
    "    y_val_label = (y_val_int == label).astype(int)\n",
    "    y_test_label = (y_test_int == label).astype(int)\n",
    "    y_val_pred_prob = clf.predict_proba(X_val_scaled)[:, label]\n",
    "    y_test_pred_prob = clf.predict_proba(X_test_scaled)[:, label]\n",
    "\n",
    "    fpr_val, tpr_val, _ = roc_curve(y_val_label, y_val_pred_prob)\n",
    "    fpr_test, tpr_test, _ = roc_curve(y_test_label, y_test_pred_prob)\n",
    "\n",
    "    print(f\"Validation ROC AUC for class {group_name} (Class {label}): {auc(fpr_val, tpr_val)}\")\n",
    "    print(f\"Test ROC AUC for class {group_name} (Class {label}): {auc(fpr_test, tpr_test)}\")\n",
    "\n",
    "y_val_pred = le.inverse_transform(clf.predict(X_val_scaled))\n",
    "y_test_pred = le.inverse_transform(clf.predict(X_test_scaled))\n",
    "\n",
    "print(\"\\nValidation Precision, Recall, F1-score:\\n\", classification_report(y_val, y_val_pred))\n",
    "print(\"\\nTest Precision, Recall, F1-score:\\n\", classification_report(y_test, y_test_pred))\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, roc_curve, auc\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "# The features are 'stationary_ratio' and 'Tik-norm'\n",
    "X = data[list(features.keys())]\n",
    "\n",
    "# The target is 'Group'\n",
    "y = data['Group']\n",
    "\n",
    "# Split the data into train+validation set and test set\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Further split the train data into train set and validation set\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.5, random_state=42)\n",
    "\n",
    "scaler = RobustScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Encode labels\n",
    "le = LabelEncoder()\n",
    "y_train_int = le.fit_transform(y_train)\n",
    "y_val_int = le.transform(y_val)\n",
    "y_test_int = le.transform(y_test)\n",
    "\n",
    "clf = SVC(probability=True, kernel='linear', random_state=42) # You can change the kernel as needed\n",
    "clf.fit(X_train_scaled, y_train_int)\n",
    "\n",
    "# Validate and test metrics\n",
    "for label, group_name in enumerate(le.classes_):\n",
    "    y_val_label = (y_val_int == label).astype(int)\n",
    "    y_test_label = (y_test_int == label).astype(int)\n",
    "    y_val_pred_prob = clf.predict_proba(X_val_scaled)[:, label]\n",
    "    y_test_pred_prob = clf.predict_proba(X_test_scaled)[:, label]\n",
    "\n",
    "    fpr_val, tpr_val, _ = roc_curve(y_val_label, y_val_pred_prob)\n",
    "    fpr_test, tpr_test, _ = roc_curve(y_test_label, y_test_pred_prob)\n",
    "\n",
    "    print(f\"Validation ROC AUC for class {group_name} (Class {label}): {auc(fpr_val, tpr_val)}\")\n",
    "    print(f\"Test ROC AUC for class {group_name} (Class {label}): {auc(fpr_test, tpr_test)}\")\n",
    "\n",
    "y_val_pred = le.inverse_transform(clf.predict(X_val_scaled))\n",
    "y_test_pred = le.inverse_transform(clf.predict(X_test_scaled))\n",
    "\n",
    "print(\"\\nValidation Precision, Recall, F1-score:\\n\", classification_report(y_val, y_val_pred))\n",
    "print(\"\\nTest Precision, Recall, F1-score:\\n\", classification_report(y_test, y_test_pred))\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, roc_curve, auc\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "# The features are 'stationary_ratio' and 'Tik-norm'\n",
    "X = data[list(features.keys())]\n",
    "\n",
    "# The target is 'Group'\n",
    "y = data['Group']\n",
    "\n",
    "# Split the data into train+validation set and test set\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Further split the train data into train set and validation set\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.5, random_state=42)\n",
    "\n",
    "scaler = RobustScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Encode labels\n",
    "le = LabelEncoder()\n",
    "y_train_int = le.fit_transform(y_train)\n",
    "y_val_int = le.transform(y_val)\n",
    "y_test_int = le.transform(y_test)\n",
    "\n",
    "clf = GaussianNB()\n",
    "clf.fit(X_train_scaled, y_train_int)\n",
    "\n",
    "# Validate and test metrics\n",
    "for label, group_name in enumerate(le.classes_):\n",
    "    y_val_label = (y_val_int == label).astype(int)\n",
    "    y_test_label = (y_test_int == label).astype(int)\n",
    "    y_val_pred_prob = clf.predict_proba(X_val_scaled)[:, label]\n",
    "    y_test_pred_prob = clf.predict_proba(X_test_scaled)[:, label]\n",
    "\n",
    "    fpr_val, tpr_val, _ = roc_curve(y_val_label, y_val_pred_prob)\n",
    "    fpr_test, tpr_test, _ = roc_curve(y_test_label, y_test_pred_prob)\n",
    "\n",
    "    print(f\"Validation ROC AUC for class {group_name} (Class {label}): {auc(fpr_val, tpr_val)}\")\n",
    "    print(f\"Test ROC AUC for class {group_name} (Class {label}): {auc(fpr_test, tpr_test)}\")\n",
    "\n",
    "y_val_pred = le.inverse_transform(clf.predict(X_val_scaled))\n",
    "y_test_pred = le.inverse_transform(clf.predict(X_test_scaled))\n",
    "\n",
    "print(\"\\nValidation Precision, Recall, F1-score:\\n\", classification_report(y_val, y_val_pred))\n",
    "print(\"\\nTest Precision, Recall, F1-score:\\n\", classification_report(y_test, y_test_pred))\n",
    "\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, RobustScaler\n",
    "from sklearn.metrics import classification_report, roc_auc_score, accuracy_score, precision_recall_fscore_support\n",
    "from sklearn.preprocessing import label_binarize\n",
    "import pytorch_tabular\n",
    "from pytorch_tabular.models.tab_transformer import TabTransformerConfig\n",
    "from pytorch_tabular import TabularModel\n",
    "from pytorch_tabular.config import DataConfig, TrainerConfig, OptimizerConfig\n",
    "\n",
    "# Assuming you've loaded your data and created the 'Group_color' column...\n",
    "\n",
    "# Extracting features\n",
    "X = data[list(features.keys())]\n",
    "\n",
    "# Target variable\n",
    "y = data['Group']\n",
    "\n",
    "\n",
    "# Splitting data \n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# Encoding labels\n",
    "le = LabelEncoder()\n",
    "y_train_encoded = le.fit_transform(y_train)\n",
    "y_val_encoded = le.transform(y_val)\n",
    "y_test_encoded = le.transform(y_test)\n",
    "\n",
    "# Convert datasets to DataFrames\n",
    "train_df = pd.concat([X_train, pd.Series(y_train_encoded, name='Group', index=X_train.index)], axis=1)\n",
    "val_df = pd.concat([X_val, pd.Series(y_val_encoded, name='Group', index=X_val.index)], axis=1)\n",
    "test_df = pd.concat([X_test, pd.Series(y_test_encoded, name='Group', index=X_test.index)], axis=1)\n",
    "\n",
    "# Define the configurations for TabTransformer\n",
    "data_config = DataConfig(\n",
    "    target=['Group'],\n",
    "    continuous_cols=list(features.keys()),  # Assuming all features are continuous\n",
    "    categorical_cols=[]\n",
    ")\n",
    "trainer_config = TrainerConfig(\n",
    "    auto_lr_find=True,  # Automatically find a suitable learning rate\n",
    "    batch_size=1024,\n",
    "    max_epochs=1000,\n",
    "    gpus=1,\n",
    "    gradient_clip_val=1,  # Gradient clipping\n",
    "    early_stopping_patience=200  # Early stopping if validation performance doesn't improve for 5 epochs\n",
    ")\n",
    "optimizer_config = OptimizerConfig()\n",
    "\n",
    "# Define the head configuration for TabTransformer\n",
    "head_config = {}  # Use default configurations for the LinearHead\n",
    "\n",
    "# Improved TabTransformer Configuration\n",
    "model_config = TabTransformerConfig(\n",
    "    task=\"classification\",\n",
    "    head=\"LinearHead\",\n",
    "    head_config=head_config,\n",
    "    num_heads=16,\n",
    "    num_attn_blocks=16,\n",
    "    transformer_head_dim=256,\n",
    "    share_embedding=True,\n",
    "    share_embedding_strategy='fraction',\n",
    "    shared_embedding_fraction=0.5,\n",
    "    attn_dropout=0.2,\n",
    "    add_norm_dropout=0.2,\n",
    "    ff_dropout=0.2,\n",
    "    embedding_dropout=0.2,\n",
    "    batch_norm_continuous_input=True\n",
    ")\n",
    "\n",
    "tabular_model = TabularModel(\n",
    "    data_config=data_config,\n",
    "    model_config=model_config,\n",
    "    optimizer_config=optimizer_config,\n",
    "    trainer_config=trainer_config\n",
    ")\n",
    "\n",
    "# Train the model using train_df and val_df\n",
    "tabular_model.fit(train=train_df, validation=val_df)\n",
    "\n",
    "time.sleep(3) \n",
    "\n",
    "# Evaluate on test set using test_df\n",
    "test_metrics = tabular_model.evaluate(test_df)\n",
    "print(\"Test Metrics for TabTransformer Model on Test Set:\", test_metrics)\n",
    "\n",
    "# Evaluate on validation set using val_df\n",
    "val_metrics = tabular_model.evaluate(val_df)\n",
    "print(\"Test Metrics for TabTransformer Model on Validation Set:\", val_metrics)\n",
    "\n",
    "# Getting predictions on the test dataset\n",
    "test_predictions = tabular_model.predict(test_df)\n",
    "\n",
    "# Getting predictions on the validation dataset\n",
    "val_predictions = tabular_model.predict(val_df)\n",
    "\n",
    "# Extracting the predicted values for test\n",
    "test_predicted_values = test_predictions['prediction'].values\n",
    "\n",
    "# Extracting the predicted values for validation\n",
    "val_predicted_values = val_predictions['prediction'].values\n",
    "\n",
    "# Calculate metrics for both test and validation\n",
    "def calculate_metrics(y_true, predicted_values):\n",
    "    accuracy = accuracy_score(y_true, predicted_values)\n",
    "    precision, recall, fscore, _ = precision_recall_fscore_support(y_true, predicted_values, average='weighted')\n",
    "\n",
    "    if len(np.unique(y_true)) > 2:  # Multi-class case\n",
    "        y_true_binarized = label_binarize(y_true, classes=np.unique(y_true))\n",
    "        predicted_values_binarized = label_binarize(predicted_values, classes=np.unique(predicted_values))\n",
    "        roc_auc = roc_auc_score(y_true_binarized, predicted_values_binarized, average=\"weighted\", multi_class=\"ovr\")\n",
    "    else:  # Binary case\n",
    "        roc_auc = roc_auc_score(y_true, predicted_values)\n",
    "    \n",
    "    return accuracy, precision, recall, fscore, roc_auc\n",
    "\n",
    "test_accuracy, test_precision, test_recall, test_fscore, test_roc_auc = calculate_metrics(y_test_encoded, test_predicted_values)\n",
    "print(\"\\nTest Metrics:\")\n",
    "print(f\"Accuracy: {test_accuracy:.4f}\")\n",
    "print(f\"Precision: {test_precision:.4f}\")\n",
    "print(f\"Recall: {test_recall:.4f}\")\n",
    "print(f\"F1-score: {test_fscore:.4f}\")\n",
    "print(f\"ROC-AUC: {test_roc_auc:.4f}\")\n",
    "\n",
    "val_accuracy, val_precision, val_recall, val_fscore, val_roc_auc = calculate_metrics(y_val_encoded, val_predicted_values)\n",
    "print(\"\\nValidation Metrics:\")\n",
    "print(f\"Accuracy: {val_accuracy:.4f}\")\n",
    "print(f\"Precision: {val_precision:.4f}\")\n",
    "print(f\"Recall: {val_recall:.4f}\")\n",
    "print(f\"F1-score: {val_fscore:.4f}\")\n",
    "print(f\"ROC-AUC: {val_roc_auc:.4f}\")\n",
    "\n",
    "# Classification Report for Test set\n",
    "print(\"\\nClassification Report for Test Set:\")\n",
    "print(classification_report(y_test_encoded, test_predicted_values))\n",
    "\n",
    "# Classification Report for Validation set\n",
    "print(\"\\nClassification Report for Validation Set:\")\n",
    "print(classification_report(y_val_encoded, val_predicted_values))\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "from sklearn.preprocessing import LabelEncoder, RobustScaler\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "data['Group_color'] = data['Group'].map({'A': 'Group1', 'F': 'Group1', 'C': 'Group2'})\n",
    "\n",
    "# Extracting features\n",
    "X = data[list(features.keys())]\n",
    "\n",
    "# Target variable\n",
    "y = data['Group_color']\n",
    "\n",
    "# Splitting data\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.5, random_state=42)\n",
    "\n",
    "# Scaling the data\n",
    "scaler = RobustScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Grid search for KNeighborsClassifier\n",
    "param_grid_knn = {\n",
    "    'n_neighbors': [3, 5, 7, 9, 11, 13],\n",
    "    'weights': ['uniform', 'distance'],\n",
    "    'metric': ['euclidean', 'manhattan', 'minkowski', 'canberra']\n",
    "}\n",
    "grid_knn = GridSearchCV(estimator=KNeighborsClassifier(), param_grid=param_grid_knn, scoring='roc_auc', n_jobs=-1, cv=5, verbose=2)\n",
    "grid_knn.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Using the best model to predict\n",
    "y_val_pred = grid_knn.best_estimator_.predict(X_val_scaled)\n",
    "y_val_pred_proba = grid_knn.best_estimator_.predict_proba(X_val_scaled)[:, 1]\n",
    "\n",
    "# Encoding labels\n",
    "le = LabelEncoder()\n",
    "y_val_encoded = le.fit_transform(y_val)\n",
    "\n",
    "# Validation set metrics\n",
    "print(\"Validation Metrics for Best KNeighborsClassifier Model:\")\n",
    "print(\"Precision, Recall, F1-score:\\n\", classification_report(y_val, y_val_pred))\n",
    "print(\"ROC AUC Score:\", roc_auc_score(y_val_encoded, y_val_pred_proba))\n",
    "\n",
    "# Test set metrics\n",
    "y_test_pred = grid_knn.best_estimator_.predict(X_test_scaled)\n",
    "y_test_pred_proba = grid_knn.best_estimator_.predict_proba(X_test_scaled)[:, 1]\n",
    "y_test_encoded = le.transform(y_test)\n",
    "\n",
    "print(\"\\nTest Metrics for Best KNeighborsClassifier Model:\")\n",
    "print(\"Precision, Recall, F1-score:\\n\", classification_report(y_test, y_test_pred))\n",
    "print(\"ROC AUC Score:\", roc_auc_score(y_test_encoded, y_test_pred_proba))\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "from sklearn.preprocessing import LabelEncoder, RobustScaler\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "data['Group_color'] = data['Group'].map({'A': 'Group1', 'F': 'Group1', 'C': 'Group2'})\n",
    "\n",
    "# Extracting features\n",
    "X = data[list(features.keys())]\n",
    "\n",
    "# Target variable\n",
    "y = data['Group_color']\n",
    "\n",
    "# Splitting data\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.5, random_state=42)\n",
    "\n",
    "# Scaling the data\n",
    "scaler = RobustScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Grid search for RandomForestClassifier\n",
    "param_grid_rf = {\n",
    "    'n_estimators': [50, 100, 150, 200, 250, 300, 350, 400, 500],\n",
    "    'max_depth': [None, 10, 20, 30, 40, 50],\n",
    "    'min_samples_split': [2, 5, 10, 20],\n",
    "    'min_samples_leaf': [1, 2, 4, 8],\n",
    "    'bootstrap': [True, False]\n",
    "}\n",
    "\n",
    "grid_rf = GridSearchCV(estimator=RandomForestClassifier(random_state=42), param_grid=param_grid_rf, scoring='roc_auc', n_jobs=-1, cv=5, verbose=2)\n",
    "grid_rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Using the best model to predict\n",
    "y_val_pred = grid_rf.best_estimator_.predict(X_val_scaled)\n",
    "y_val_pred_proba = grid_rf.best_estimator_.predict_proba(X_val_scaled)[:, 1]\n",
    "\n",
    "# Encoding labels\n",
    "le = LabelEncoder()\n",
    "y_val_encoded = le.fit_transform(y_val)\n",
    "\n",
    "# Validation set metrics\n",
    "print(\"Validation Metrics for Best RandomForestClassifier Model:\")\n",
    "print(\"Precision, Recall, F1-score:\\n\", classification_report(y_val, y_val_pred))\n",
    "print(\"ROC AUC Score:\", roc_auc_score(y_val_encoded, y_val_pred_proba))\n",
    "\n",
    "# Test set metrics\n",
    "y_test_pred = grid_rf.best_estimator_.predict(X_test_scaled)\n",
    "y_test_pred_proba = grid_rf.best_estimator_.predict_proba(X_test_scaled)[:, 1]\n",
    "y_test_encoded = le.transform(y_test)\n",
    "\n",
    "print(\"\\nTest Metrics for Best RandomForestClassifier Model:\")\n",
    "print(\"Precision, Recall, F1-score:\\n\", classification_report(y_test, y_test_pred))\n",
    "print(\"ROC AUC Score:\", roc_auc_score(y_test_encoded, y_test_pred_proba))\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "from sklearn.preprocessing import LabelEncoder, RobustScaler\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "data['Group_color'] = data['Group'].map({'A': 'Group1', 'F': 'Group1', 'C': 'Group2'})\n",
    "\n",
    "# Extracting features\n",
    "X = data[list(features.keys())]\n",
    "\n",
    "# Target variable\n",
    "y = data['Group_color']\n",
    "\n",
    "# Encode labels to 0 and 1\n",
    "le = LabelEncoder()\n",
    "y_encoded = le.fit_transform(y)\n",
    "\n",
    "# Splitting data\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(X, y_encoded, test_size=0.3, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.5, random_state=42)\n",
    "\n",
    "# Scaling the data\n",
    "scaler = RobustScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Grid search for XGBClassifier\n",
    "param_grid_xgb = {\n",
    "    'learning_rate': [0.0001, 0.001, 0.01, 0.05, 0.1],\n",
    "    'n_estimators': [50, 100, 150, 200, 250, 300, 350, 500, 1000],\n",
    "    'max_depth': [3, 5, 7, 10, 15],\n",
    "    'gamma': [0, 0.1, 0.2, 0.3],\n",
    "    'subsample': [0.7, 0.8, 0.9, 1],\n",
    "    'colsample_bytree': [0.7, 0.8, 0.9, 1]\n",
    "}\n",
    "\n",
    "grid_xgb = GridSearchCV(estimator=XGBClassifier(random_state=42), param_grid=param_grid_xgb, scoring='roc_auc', n_jobs=-1, cv=5, verbose=2)\n",
    "grid_xgb.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Using the best model to predict\n",
    "y_val_pred = grid_xgb.best_estimator_.predict(X_val_scaled)\n",
    "y_val_pred_proba = grid_xgb.best_estimator_.predict_proba(X_val_scaled)[:, 1]\n",
    "\n",
    "# Validation set metrics\n",
    "print(\"Validation Metrics for Best XGBClassifier Model:\")\n",
    "print(\"Precision, Recall, F1-score:\\n\", classification_report(y_val, y_val_pred))\n",
    "print(\"ROC AUC Score:\", roc_auc_score(y_val, y_val_pred_proba))\n",
    "\n",
    "# Test set metrics\n",
    "y_test_pred = grid_xgb.best_estimator_.predict(X_test_scaled)\n",
    "y_test_pred_proba = grid_xgb.best_estimator_.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "print(\"\\nTest Metrics for Best XGBClassifier Model:\")\n",
    "print(\"Precision, Recall, F1-score:\\n\", classification_report(y_test, y_test_pred))\n",
    "print(\"ROC AUC Score:\", roc_auc_score(y_test, y_test_pred_proba))\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "data['Group_color'] = data['Group'].map({'A': 'Group1', 'F': 'Group1', 'C': 'Group2'})\n",
    "\n",
    "# The features are 'stationary_ratio' and 'Tik-norm'\n",
    "X = data[list(features.keys())] # Make sure the column names are correct\n",
    "\n",
    "# The target is 'Group_color'\n",
    "y = data['Group_color']\n",
    "\n",
    "# Split the data into train+validation set and test set\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Further split the train data into train set and validation set\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.5, random_state=42)\n",
    "\n",
    "scaler = RobustScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "clf = LogisticRegression(random_state=42) # You can add hyperparameters as needed\n",
    "\n",
    "clf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predict the labels of the validation set\n",
    "y_val_pred = clf.predict(X_val_scaled)\n",
    "\n",
    "# Predict the probabilities of the validation set\n",
    "y_val_pred_proba = clf.predict_proba(X_val_scaled)[:, 1]\n",
    "\n",
    "# Encode labels to 0 and 1\n",
    "le = LabelEncoder()\n",
    "y_val_encoded = le.fit_transform(y_val)\n",
    "\n",
    "# Print the accuracy and other metrics of the classifier on the validation set\n",
    "print(\"Validation Metrics:\")\n",
    "print(\"Precision, Recall, F1-score:\\n\", classification_report(y_val, y_val_pred))\n",
    "print(\"ROC AUC Score:\", roc_auc_score(y_val_encoded, y_val_pred_proba))\n",
    "\n",
    "# Predict the labels of the test set\n",
    "y_test_pred = clf.predict(X_test_scaled)\n",
    "\n",
    "# Predict the probabilities of the test set\n",
    "y_test_pred_proba = clf.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "# Encode labels to 0 and 1\n",
    "y_test_encoded = le.transform(y_test)\n",
    "\n",
    "# Print the accuracy and other metrics of the classifier on the test set\n",
    "print(\"\\nTest Metrics:\")\n",
    "print(\"Precision, Recall, F1-score:\\n\", classification_report(y_test, y_test_pred))\n",
    "print(\"ROC AUC Score:\", roc_auc_score(y_test_encoded, y_test_pred_proba))\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "data['Group_color'] = data['Group'].map({'A': 'Group1', 'F': 'Group1', 'C': 'Group2'})\n",
    "\n",
    "# The features are 'stationary_ratio' and 'Tik-norm'\n",
    "X = data[list(features.keys())] # Make sure the column names are correct\n",
    "\n",
    "# The target is 'Group_color'\n",
    "y = data['Group_color']\n",
    "\n",
    "# Split the data into train+validation set and test set\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Further split the train data into train set and validation set\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.5, random_state=42)\n",
    "\n",
    "scaler = RobustScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "clf = SVC(random_state=42, probability=True) # You can add hyperparameters as needed\n",
    "\n",
    "clf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predict the labels of the validation set\n",
    "y_val_pred = clf.predict(X_val_scaled)\n",
    "\n",
    "# Predict the probabilities of the validation set\n",
    "y_val_pred_proba = clf.predict_proba(X_val_scaled)[:, 1]\n",
    "\n",
    "# Encode labels to 0 and 1\n",
    "le = LabelEncoder()\n",
    "y_val_encoded = le.fit_transform(y_val)\n",
    "\n",
    "# Print the accuracy and other metrics of the classifier on the validation set\n",
    "print(\"Validation Metrics:\")\n",
    "print(\"Precision, Recall, F1-score:\\n\", classification_report(y_val, y_val_pred))\n",
    "print(\"ROC AUC Score:\", roc_auc_score(y_val_encoded, y_val_pred_proba))\n",
    "\n",
    "# Predict the labels of the test set\n",
    "y_test_pred = clf.predict(X_test_scaled)\n",
    "\n",
    "# Predict the probabilities of the test set\n",
    "y_test_pred_proba = clf.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "# Encode labels to 0 and 1\n",
    "y_test_encoded = le.transform(y_test)\n",
    "\n",
    "# Print the accuracy and other metrics of the classifier on the test set\n",
    "print(\"\\nTest Metrics:\")\n",
    "print(\"Precision, Recall, F1-score:\\n\", classification_report(y_test, y_test_pred))\n",
    "print(\"ROC AUC Score:\", roc_auc_score(y_test_encoded, y_test_pred_proba))\n",
    "\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, RobustScaler\n",
    "from sklearn.metrics import classification_report, roc_auc_score, accuracy_score, precision_recall_fscore_support\n",
    "from sklearn.preprocessing import label_binarize\n",
    "import pytorch_tabular\n",
    "from pytorch_tabular.models.tab_transformer import TabTransformerConfig\n",
    "from pytorch_tabular import TabularModel\n",
    "from pytorch_tabular.config import DataConfig, TrainerConfig, OptimizerConfig\n",
    "\n",
    "# Assuming you've loaded your data and created the 'Group_color' column...\n",
    "\n",
    "# Extracting features\n",
    "X = data[list(features.keys())]\n",
    "# Target variable\n",
    "y = data['Group_color']\n",
    "\n",
    "# Splitting data \n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# Encoding labels\n",
    "le = LabelEncoder()\n",
    "y_train_encoded = le.fit_transform(y_train)\n",
    "y_val_encoded = le.transform(y_val)\n",
    "y_test_encoded = le.transform(y_test)\n",
    "\n",
    "# Convert datasets to DataFrames\n",
    "train_df = pd.concat([X_train, pd.Series(y_train_encoded, name='Group_color', index=X_train.index)], axis=1)\n",
    "val_df = pd.concat([X_val, pd.Series(y_val_encoded, name='Group_color', index=X_val.index)], axis=1)\n",
    "test_df = pd.concat([X_test, pd.Series(y_test_encoded, name='Group_color', index=X_test.index)], axis=1)\n",
    "\n",
    "# Define the configurations for TabTransformer\n",
    "data_config = DataConfig(\n",
    "    target=['Group_color'],\n",
    "    continuous_cols=list(features.keys()),  # Assuming all features are continuous\n",
    "    categorical_cols=[]\n",
    ")\n",
    "trainer_config = TrainerConfig(\n",
    "    auto_lr_find=True,  # Automatically find a suitable learning rate\n",
    "    batch_size=1024,\n",
    "    max_epochs=100_000,\n",
    "    gpus=1,\n",
    "    gradient_clip_val=1,  # Gradient clipping\n",
    "    early_stopping_patience=100  # Early stopping if validation performance doesn't improve for 5 epochs\n",
    ")\n",
    "optimizer_config = OptimizerConfig()\n",
    "\n",
    "# Define the head configuration for TabTransformer\n",
    "head_config = {}  # Use default configurations for the LinearHead\n",
    "\n",
    "# Improved TabTransformer Configuration\n",
    "model_config = TabTransformerConfig(\n",
    "    task=\"classification\",\n",
    "    head=\"LinearHead\",\n",
    "    head_config=head_config,\n",
    "    num_heads=8,\n",
    "    num_attn_blocks=8,\n",
    "    transformer_head_dim=256,\n",
    "    share_embedding=True,\n",
    "    share_embedding_strategy='fraction',\n",
    "    shared_embedding_fraction=0.5,\n",
    "    attn_dropout=0.2,\n",
    "    add_norm_dropout=0.2,\n",
    "    ff_dropout=0.2,\n",
    "    embedding_dropout=0.2,\n",
    "    batch_norm_continuous_input=True\n",
    ")\n",
    "\n",
    "tabular_model = TabularModel(\n",
    "    data_config=data_config,\n",
    "    model_config=model_config,\n",
    "    optimizer_config=optimizer_config,\n",
    "    trainer_config=trainer_config\n",
    ")\n",
    "\n",
    "# Train the model using train_df and val_df\n",
    "tabular_model.fit(train=train_df, validation=val_df)\n",
    "\n",
    "time.sleep(3) \n",
    "\n",
    "# Evaluate on test set using test_df\n",
    "test_metrics = tabular_model.evaluate(test_df)\n",
    "print(\"Test Metrics for TabTransformer Model on Test Set:\", test_metrics)\n",
    "\n",
    "# Evaluate on validation set using val_df\n",
    "val_metrics = tabular_model.evaluate(val_df)\n",
    "print(\"Test Metrics for TabTransformer Model on Validation Set:\", val_metrics)\n",
    "\n",
    "# Getting predictions on the test dataset\n",
    "test_predictions = tabular_model.predict(test_df)\n",
    "\n",
    "# Getting predictions on the validation dataset\n",
    "val_predictions = tabular_model.predict(val_df)\n",
    "\n",
    "# Extracting the predicted values for test\n",
    "test_predicted_values = test_predictions['prediction'].values\n",
    "\n",
    "# Extracting the predicted values for validation\n",
    "val_predicted_values = val_predictions['prediction'].values\n",
    "\n",
    "# Calculate metrics for both test and validation\n",
    "def calculate_metrics(y_true, predicted_values):\n",
    "    accuracy = accuracy_score(y_true, predicted_values)\n",
    "    precision, recall, fscore, _ = precision_recall_fscore_support(y_true, predicted_values, average='weighted')\n",
    "\n",
    "    if len(np.unique(y_true)) > 2:  # Multi-class case\n",
    "        y_true_binarized = label_binarize(y_true, classes=np.unique(y_true))\n",
    "        predicted_values_binarized = label_binarize(predicted_values, classes=np.unique(predicted_values))\n",
    "        roc_auc = roc_auc_score(y_true_binarized, predicted_values_binarized, average=\"weighted\", multi_class=\"ovr\")\n",
    "    else:  # Binary case\n",
    "        roc_auc = roc_auc_score(y_true, predicted_values)\n",
    "    \n",
    "    return accuracy, precision, recall, fscore, roc_auc\n",
    "\n",
    "test_accuracy, test_precision, test_recall, test_fscore, test_roc_auc = calculate_metrics(y_test_encoded, test_predicted_values)\n",
    "print(\"\\nTest Metrics:\")\n",
    "print(f\"Accuracy: {test_accuracy:.4f}\")\n",
    "print(f\"Precision: {test_precision:.4f}\")\n",
    "print(f\"Recall: {test_recall:.4f}\")\n",
    "print(f\"F1-score: {test_fscore:.4f}\")\n",
    "print(f\"ROC-AUC: {test_roc_auc:.4f}\")\n",
    "\n",
    "val_accuracy, val_precision, val_recall, val_fscore, val_roc_auc = calculate_metrics(y_val_encoded, val_predicted_values)\n",
    "print(\"\\nValidation Metrics:\")\n",
    "print(f\"Accuracy: {val_accuracy:.4f}\")\n",
    "print(f\"Precision: {val_precision:.4f}\")\n",
    "print(f\"Recall: {val_recall:.4f}\")\n",
    "print(f\"F1-score: {val_fscore:.4f}\")\n",
    "print(f\"ROC-AUC: {val_roc_auc:.4f}\")\n",
    "\n",
    "# Classification Report for Test set\n",
    "print(\"\\nClassification Report for Test Set:\")\n",
    "print(classification_report(y_test_encoded, test_predicted_values))\n",
    "\n",
    "# Classification Report for Validation set\n",
    "print(\"\\nClassification Report for Validation Set:\")\n",
    "print(classification_report(y_val_encoded, val_predicted_values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24718614-65ec-4cd1-94e5-22ac6c7408cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
